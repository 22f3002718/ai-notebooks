{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNU3tcTpTQgY6+x8yTM4XuI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/22f3002718/ai-notebooks/blob/main/starcoder2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dlRzqEo1sB1-"
      },
      "outputs": [],
      "source": [
        "!pip install -q transformers accelerate\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "\n",
        "model_id = \"bigcode/starcoder2-3b\"\n",
        "\n",
        "# Load tokenizer (converts text to tokens and back)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "\n",
        "# Load model (uses float16 to run efficiently on Colab GPUs)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    device_map=\"auto\",           # Automatically puts it on GPU\n",
        "    torch_dtype=torch.float16    # Efficient precision\n",
        ")\n",
        "model.eval()\n"
      ],
      "metadata": {
        "id": "3vhatCeVsNrn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"def is_prime(n):\\n    \\\"\\\"\\\"Check if a number is prime.\\\"\\\"\\\"\\n\"\n",
        "\n",
        "# Tokenize the prompt\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "# Generate complete function\n",
        "with torch.no_grad():\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=128,                     # enough tokens for logic\n",
        "        do_sample=True,                         # enables randomness\n",
        "        temperature=0.7,                        # balance creativity\n",
        "        top_k=50,\n",
        "        eos_token_id=tokenizer.eos_token_id,    # stop properly\n",
        "        pad_token_id=tokenizer.eos_token_id     # avoid warning\n",
        "    )\n",
        "\n",
        "# Decode the output tokens to readable text\n",
        "generated_code = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "# Print only the generated part, ignoring repeat of the prompt\n",
        "print(\"\\nüß† Generated Code:\\n\")\n",
        "print(generated_code[len(prompt):].strip())\n"
      ],
      "metadata": {
        "id": "1l7JWAmrsT60"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run(prompt, max_tokens=100):\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(**inputs, max_new_tokens=max_tokens, do_sample=False)\n",
        "    decoded = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    return decoded.strip()\n",
        "\n",
        "# 1Ô∏è‚É£ LENGTH & LATENCY\n",
        "print(\"üîç Length + Latency Test\")\n",
        "for length in [128, 256, 512, 768, 1024]:\n",
        "    test_prompt = \"def example():\\n    # \" + \"This is a comment.\\n    # \" * (length // 10)\n",
        "    try:\n",
        "        start = time.time()\n",
        "        output = run(test_prompt, max_tokens=20)\n",
        "        end = time.time()\n",
        "        print(f\"‚úÖ {length} tokens ‚Üí Success | ‚è± {end - start:.2f}s\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå {length} tokens ‚Üí Failure | üí• {e}\")\n",
        "        break\n",
        "\n",
        "# 2Ô∏è‚É£ COMPLEXITY TEST\n",
        "print(\"\\nüîç Complexity Prompt Test\")\n",
        "prompts = [\n",
        "    \"# Add two numbers\",                           # simple\n",
        "    \"# Do something useful\",                       # vague\n",
        "    \"# Implement greatness\",                       # abstract\n",
        "]\n",
        "\n",
        "for p in prompts:\n",
        "    out = run(p, max_tokens=60)\n",
        "    success = \"def\" in out or \"=\" in out\n",
        "    status = \"‚úÖ Success\" if success else \"‚ùå Failure\"\n",
        "    print(f\"\\nPrompt: {p}\\n{status}\\nüìù Output:\\n{out}\")\n",
        "\n",
        "# 3Ô∏è‚É£ STABILITY TEST (minor changes to prompt)\n",
        "print(\"\\nüîç Prompt Stability Test\")\n",
        "base = \"def reverse_string(s):\"\n",
        "variation = \"def reverse_str(s):\"\n",
        "\n",
        "out1 = run(base, 60)\n",
        "out2 = run(variation, 60)\n",
        "\n",
        "same = out1.strip().splitlines()[:2] == out2.strip().splitlines()[:2]\n",
        "status = \"‚úÖ Stable\" if same else \"‚ùå Unstable\"\n",
        "print(f\"\\nPrompt 1: {base}\\nüìù {out1}\\n\\nPrompt 2: {variation}\\nüìù {out2}\\n{status}\")\n",
        "\n",
        "# 4Ô∏è‚É£ AUTOCOMPLETION TEST\n",
        "print(\"\\nüîç Code Autocompletion Test\")\n",
        "prompt = \"def factorial(n):\\n    \\\"\\\"\\\"Returns factorial\\\"\\\"\\\"\\n    if n == 0:\"\n",
        "out = run(prompt, 60)\n",
        "success = \"return\" in out and \"*\" in out\n",
        "status = \"‚úÖ Success\" if success else \"‚ùå Failure\"\n",
        "print(f\"Prompt:\\n{prompt}\\n\\nüìù Output:\\n{out}\\n{status}\")\n",
        "\n",
        "# 5Ô∏è‚É£ SYNTAX CHECK TEST (does it produce valid Python code?)\n",
        "print(\"\\nüîç Syntax Check Test\")\n",
        "\n",
        "import ast\n",
        "\n",
        "def is_valid_syntax(code):\n",
        "    try:\n",
        "        ast.parse(code)\n",
        "        return True\n",
        "    except:\n",
        "        return False\n",
        "\n",
        "prompt = \"def is_even(n):\\n    return n % 2 == 0\"\n",
        "output = run(prompt, 10)\n",
        "syntax_ok = is_valid_syntax(output)\n",
        "status = \"‚úÖ Valid Python\" if syntax_ok else \"‚ùå Invalid\"\n",
        "print(f\"\\nüìù Output:\\n{output}\\n{status}\")\n"
      ],
      "metadata": {
        "id": "cmAH2-lesmf2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ‚úÖ Install Transformers if not already\n",
        "!pip install -q transformers accelerate\n",
        "\n",
        "# ‚úÖ Load StarCoder2-3B from Hugging Face\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import torch\n",
        "\n",
        "model_id = \"bigcode/starcoder2-3b\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=torch.float16,\n",
        "    trust_remote_code=True\n",
        ")\n",
        "\n",
        "# ‚úÖ Code generation function\n",
        "def generate_code(prompt, max_new_tokens=128):\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=max_new_tokens,\n",
        "            do_sample=True,\n",
        "            temperature=0.7,\n",
        "            pad_token_id=tokenizer.eos_token_id,\n",
        "            eos_token_id=tokenizer.eos_token_id\n",
        "        )\n",
        "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "# ‚úÖ Prompts to test: Python, Java, Go, React\n",
        "prompts = {\n",
        "    \"Python\": \"def fibonacci(n):\",\n",
        "    \"Java\": \"public class Factorial {\\n    public static int factorial(int n) {\",\n",
        "    \"Go\": \"package main\\nimport \\\"fmt\\\"\\n\\nfunc isPrime(n int) bool {\",\n",
        "    \"React (JSX)\": \"import React, { useState } from 'react';\\n\\nfunction Counter() {\"\n",
        "}\n",
        "\n",
        "# ‚úÖ Run generation for each language\n",
        "for lang, prompt in prompts.items():\n",
        "    print(f\"\\nüß† {lang} Prompt:\\n{prompt}\")\n",
        "    code = generate_code(prompt)\n",
        "    print(f\"\\nüíª {lang} Output:\\n{code}\")\n"
      ],
      "metadata": {
        "id": "jBYnOvQftq-0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "B2NiUvbucLVh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}